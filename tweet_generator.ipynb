{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZILRsxCDR3xR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", encoding='latin-1')\n",
        "data.columns = ['target', 'id', 'date', 'query', 'user', 'tweet']"
      ],
      "metadata": {
        "id": "AAZh1wwDUEw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "waOWD6_bf83A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "BvqkvT1UVxaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess a single tweet\n",
        "def preprocess_tweet(tweet):\n",
        "    # Remove URLs, mentions, and special characters\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|@\\S+|\\W\", \" \", tweet)\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "    # Tokenize the tweet\n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "    # Remove stop words and perform stemming\n",
        "    preprocessed_tweet = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    preprocessed_tweet = ' '.join(preprocessed_tweet)\n",
        "    return preprocessed_tweet"
      ],
      "metadata": {
        "id": "DyzWdVMNahyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess all tweets\n",
        "data['preprocessed_tweet'] = data['tweet'].apply(preprocess_tweet)"
      ],
      "metadata": {
        "id": "W4b9mUcfapAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform(data['preprocessed_tweet'])"
      ],
      "metadata": {
        "id": "L3wlEuipazP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, data['tweet'], test_size=0.2, random_state=42)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "g72K8U5La65P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def generate_tweet(seed_sentence):\n",
        "    seed_sentence = preprocess_tweet(seed_sentence)\n",
        "    seed_feature = vectorizer.transform([seed_sentence])\n",
        "    predicted_target = model.predict(seed_feature)\n",
        "    generated_target = int(predicted_target[0])\n",
        "    # Filter tweets with the generated target\n",
        "    filtered_tweets = data[data['target'] == generated_target]['tweet']\n",
        "    if len(filtered_tweets) == 0:\n",
        "        return \"No matching tweet found for the generated target.\"\n",
        "    else:\n",
        "        # Randomly select a tweet from the filtered tweets\n",
        "        generated_tweet = filtered_tweets.sample().iloc[0]\n",
        "        return generated_tweet\n",
        "        \n",
        "        '''\n",
        "\n",
        "def generate_tweet(seed_sentence):\n",
        "    seed_sentence = preprocess_tweet(seed_sentence)\n",
        "    seed_feature = vectorizer.transform([seed_sentence])\n",
        "    predicted_tweet = model.predict(seed_feature)\n",
        "    return predicted_tweet[0]        \n",
        "        "
      ],
      "metadata": {
        "id": "B1EC9Jx7bf-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_sentence = 'the '\n",
        "generated_tweet = generate_tweet(seed_sentence)\n",
        "print(\"Generated target:\", generated_tweet)"
      ],
      "metadata": {
        "id": "HFdgJJNVbtSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(reference, candidate):\n",
        "    reference = reference.split()\n",
        "    candidate = candidate.split()\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "def calculate_rouge_scores(reference, candidate):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(candidate, reference)[0]\n",
        "    return scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']\n",
        "\n",
        "# Generate a set of tweets and evaluate them\n",
        "reference_tweets = data['tweet'].sample(100)  # Select a subset of reference tweets\n",
        "generated_tweets = []\n",
        "for reference_tweet in reference_tweets:\n",
        "    seed_sentence = reference_tweet[:20]  # Use the first 20 characters as the seed\n",
        "    generated_tweet = generate_tweet(seed_sentence)\n",
        "    generated_tweets.append(generated_tweet)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "bleu_scores = []\n",
        "rouge_1_scores = []\n",
        "rouge_2_scores = []\n",
        "rouge_l_scores = []\n",
        "\n",
        "for reference_tweet, generated_tweet in zip(reference_tweets, generated_tweets):\n",
        "        # Calculate evaluation metrics\n",
        "    bleu_score = calculate_bleu_score(reference_tweet, generated_tweet)\n",
        "    rouge_1_score, rouge_2_score, rouge_l_score = calculate_rouge_scores(reference_tweet, generated_tweet)\n",
        "\n",
        "    # Append scores to the lists\n",
        "    bleu_scores.append(bleu_score)\n",
        "    rouge_1_scores.append(rouge_1_score)\n",
        "    rouge_2_scores.append(rouge_2_score)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "# Calculate average scores\n",
        "average_bleu_score = np.mean(bleu_scores)\n",
        "average_rouge_1_score = np.mean(rouge_1_scores)\n",
        "average_rouge_2_score = np.mean(rouge_2_scores)\n",
        "average_rouge_l_score = np.mean(rouge_l_scores)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average BLEU Score:\", average_bleu_score)\n",
        "print(\"Average ROUGE-1 Score:\", average_rouge_1_score)\n",
        "print(\"Average ROUGE-2 Score:\", average_rouge_2_score)\n",
        "print(\"Average ROUGE-L Score:\", average_rouge_l_score)"
      ],
      "metadata": {
        "id": "gJeD9cDBb4oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}